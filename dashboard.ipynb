{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dandi\n",
    "from dandi.dandiapi import DandiAPIClient \n",
    "# panda\n",
    "import pandas as pd\n",
    "# bokeh\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, TapTool, CustomJS, HoverTool\n",
    "from bokeh.io import output_notebook, output_file\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Category20, Category10\n",
    "# pickle\n",
    "import pickle \n",
    " # os\n",
    "import os\n",
    "# json\n",
    "import json \n",
    "# urllib\n",
    "from urllib.parse import quote\n",
    "# jinja2\n",
    "from jinja2 import Environment, FileSystemLoader \n",
    "\n",
    "\n",
    "# For rendering in Jupyter/Notebook environments\n",
    "output_notebook()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating esstential directories\n",
    "# directory to save pickle objects\n",
    "object_dict = \"./objs\"\n",
    "# create directory\n",
    "try:\n",
    "    os.mkdir(object_dict)\n",
    "except:\n",
    "    print(f\"Note: {object_dict} to store pickle objects already exist.\")\n",
    "\n",
    "# plots directory\n",
    "plots_dict = \"./plots\"\n",
    "# Create the directory \n",
    "try:\n",
    "    os.mkdir(plots_dict)\n",
    "except:\n",
    "    print(f\"Note: {plots_dict} to store plots objects already exist.\")\n",
    "\n",
    "# Dandi dataset ID\n",
    "dandi_set = \"000026\"\n",
    "\n",
    "# API call\n",
    "api = \"https://api.dandiarchive.org/api\"\n",
    "dandi_api = DandiAPIClient(api)\n",
    "\n",
    "# Getting the dataset from the dandi server\n",
    "dandi_dataset = dandi_api.get_dandiset(dandi_set)\n",
    "\n",
    "# Saving the dandi_dataset object\n",
    "with open(f'{object_dict}/dandiset.pkl', 'wb') as file:\n",
    "    pickle.dump(dandi_dataset, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Section: 1. Data Gathering ########################\n",
    "\n",
    "\n",
    "def extract_subject_from_path(path_parts):\n",
    "    \"\"\"\n",
    "    Extract the subject ID from the parts of the path if the directory contains 'sub-' (subject identifier).\n",
    "    Assumes the path follows a structure where subject IDs are in directories like 'sub-01', 'sub-02', etc.\n",
    "\n",
    "    Args:\n",
    "        path_parts (list): List of directory and file parts obtained from splitting the path by '/'.\n",
    "\n",
    "    Returns:\n",
    "        str: Subject ID if found, otherwise None.\n",
    "    \"\"\"\n",
    "    for val in path_parts[:-1]:  # Loop through all parts except the last (which is usually the file name)\n",
    "        if val.startswith(\"sub-\"):  # Check if part starts with 'sub-'\n",
    "            return val.split(\"sub-\")[1]  # Return the part after 'sub-'\n",
    "    return None  # If no 'sub-' part found, return None\n",
    "\n",
    "\n",
    "def parse_asset_filename(assetname):\n",
    "    \"\"\"\n",
    "    Extract key-value pairs from the asset file name. The file name parts are assumed to follow\n",
    "    a 'key-value' format, where the key and value are separated by a hyphen (e.g., 'sub-01', 'task-rest').\n",
    "    \n",
    "    Args:\n",
    "        assetname (str): The file name of the asset.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary of key-value pairs extracted from the file name.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        key_value.split(\"-\")[0]: \"-\".join(key_value.split(\"-\")[1:])  # Split key-value pairs by '-'\n",
    "        for key_value in assetname.split(\".\")[0].split(\"_\")  # First, remove the file extension, then split by '_'\n",
    "        if \"-\" in key_value  # Only keep parts that contain a '-'\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_modality_from_filename(assetname, assetpath):\n",
    "    \"\"\"\n",
    "    Extract the modality information from the file name. The modality usually appears at the end\n",
    "    of the file name, separated by underscores (e.g., 'sub-01_task-rest_bold.nii' -> 'bold').\n",
    "\n",
    "    Args:\n",
    "        assetname (str): The name of the file.\n",
    "        assetpath (str): The full path of the asset.\n",
    "\n",
    "    Returns:\n",
    "        str: The modality if found, otherwise None.\n",
    "    \"\"\"\n",
    "    if \"_\" in assetname and \"sub-\" in assetname:  # Check if the file name follows the expected format\n",
    "        # Extract everything after 'sub-' and split by '/'\n",
    "        path = \"sub-\".join(assetpath.split(\"sub-\")[1:])\n",
    "        if len(path.split(\"/\")) > 1:  # If there are multiple parts in the path, assume it's valid\n",
    "            return assetname.split(\"_\")[-1].split(\".\")[0]  # Extract modality from the last part of the name\n",
    "    return None  # Return None if no modality found\n",
    "\n",
    "\n",
    "def assets_to_df(ds):\n",
    "    \"\"\"\n",
    "    Convert assets from a dandiset into a structured pandas DataFrame with extracted metadata.\n",
    "\n",
    "    Args:\n",
    "        ds: The dandiset object obtained from the Dandi API.\n",
    "\n",
    "    Returns:\n",
    "        df (pandas.DataFrame): A DataFrame containing information about each asset.\n",
    "        assets (list): A list of asset objects from the dandiset.\n",
    "    \"\"\"\n",
    "    # Get the list of assets from the dataset\n",
    "    assets = list(ds.get_assets())\n",
    "    \n",
    "    # Initialize an empty list to store metadata for each asset\n",
    "    asset_info = []\n",
    "    \n",
    "    # Loop through each asset in the list\n",
    "    for asset in assets:\n",
    "        # Split the asset's path into parts (directories and file name)\n",
    "        path_parts = asset.path.split(\"/\")\n",
    "        \n",
    "        # Extract the subject ID (subdir) from the path, if available\n",
    "        sub = extract_subject_from_path(path_parts)\n",
    "        \n",
    "        # Extract the file name from the path (the last part)\n",
    "        assetname = path_parts[-1]\n",
    "        \n",
    "        # Parse key-value pairs from the file name (e.g., 'sub-01_task-rest')\n",
    "        info = parse_asset_filename(assetname)\n",
    "        \n",
    "        # If a subject ID was found, add it to the metadata dictionary\n",
    "        if sub:\n",
    "            info[\"subdir\"] = sub\n",
    "        \n",
    "        # Add the full path to the metadata dictionary\n",
    "        info[\"path\"] = asset.path\n",
    "        \n",
    "        # Extract modality information (e.g., 'bold', 'T1w') from the file name\n",
    "        modality = extract_modality_from_filename(assetname, asset.path)\n",
    "        if modality:\n",
    "            info[\"modality\"] = modality\n",
    "        \n",
    "        # Extract the file extension (e.g., 'nii', 'bvec') and add it to the dictionary\n",
    "        ext = \".\".join(assetname.split(\".\")[1:])\n",
    "        info[\"extension\"] = ext\n",
    "        \n",
    "        # Add the asset's modified date to the metadata dictionary\n",
    "        info[\"modified\"] = asset.modified\n",
    "        \n",
    "        # Append the metadata dictionary for this asset to the list\n",
    "        asset_info.append(info)\n",
    "    \n",
    "    # Convert the list of asset metadata into a pandas DataFrame\n",
    "    df = pd.DataFrame(asset_info)\n",
    "    \n",
    "    # Return both the DataFrame and the original list of assets\n",
    "    return df, assets\n",
    "\n",
    "\n",
    "\n",
    "######################## Sections: 2. Generating Modality X Subject Plot\n",
    "#                                                       &\n",
    "#                                                     6. Generating Stain X Sample Interactive Plots ########################\n",
    "\n",
    "\n",
    "def generate_plot(data, title, save_path, interactive=True):\n",
    "    \"\"\"\n",
    "    Generate a Bokeh plot for visualizing subject-modality or sample-stain relationships.\n",
    "\n",
    "    Args:\n",
    "    data (pandas.DataFrame): DataFrame containing 'sample', 'stain', and optionally 'url'.\n",
    "        - 'sample' refers to the x-axis values (e.g., subjects).\n",
    "        - 'stain' refers to the y-axis values (e.g., modalities).\n",
    "        - 'url' (optional) is used for interactive plots where clicking on a rectangle opens a URL.\n",
    "    title (str): Title of the plot, which appears at the top.\n",
    "    save_path (str): The path to save the generated plot as an HTML file.\n",
    "    interactive (bool): If True, enables interactive features (e.g., clicking to open URLs and hover tooltips). \n",
    "                        If False, generates a non-interactive plot.\n",
    "\n",
    "    Returns:\n",
    "    None: The plot is displayed in the browser and saved to the provided `save_path`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a Bokeh ColumnDataSource from the given DataFrame.\n",
    "    # ColumnDataSource is the Bokeh format for binding data to plots.\n",
    "    source = ColumnDataSource(data)\n",
    "\n",
    "    # Extract unique values for 'sample' (x-axis) and 'stain' (y-axis) from the DataFrame.\n",
    "    # These unique values are used to define the range of x and y axes.\n",
    "    x_range = list(data['sample'].unique())\n",
    "    y_range = list(data['stain'].unique())\n",
    "\n",
    "    # Determine the number of unique stains (or modalities) to apply distinct colors.\n",
    "    num_stains = len(y_range)\n",
    "    \n",
    "    # Use Category20 palette for up to 20 unique values; otherwise, cycle through Category10 palette.\n",
    "    palette = Category20[num_stains] if num_stains <= 20 else Category10[num_stains % 10]\n",
    "\n",
    "    # Create the Bokeh figure object. This is where the plot settings are defined.\n",
    "    p = figure(\n",
    "        title=title,  # Title of the plot\n",
    "        x_range=x_range,  # Set the x-axis range (samples)\n",
    "        y_range=y_range,  # Set the y-axis range (stains/modalities)\n",
    "        tools=\"tap\" if interactive else \"save\",  # Use 'tap' tool only if interactive, otherwise just 'save' tool.\n",
    "        width=1200,   # Width of the plot, adjusted to display more samples comfortably\n",
    "        height=600,   # Height of the plot, adjusted to display multiple stains/modalities\n",
    "        toolbar_location=\"right\"  # Place toolbar (e.g., save button) on the right side for better layout\n",
    "    )\n",
    "\n",
    "    # Add rectangles to represent each sample-stain (subject-modality) combination.\n",
    "    # Rectangles represent the cells in the matrix (e.g., a specific subject with a specific modality).\n",
    "    p.rect(\n",
    "        x=\"sample\",  # Set the x-axis to 'sample' values (e.g., subjects)\n",
    "        y=\"stain\",   # Set the y-axis to 'stain' values (e.g., modalities)\n",
    "        width=0.9,   # Set the width of each rectangle (close to 1 to fill the space, but with slight spacing)\n",
    "        height=0.9,  # Set the height of each rectangle\n",
    "        source=source,  # Provide the data source that contains the x, y values and possibly URLs\n",
    "        fill_color=factor_cmap('stain', palette=palette, factors=y_range),  # Assign colors based on 'stain'\n",
    "        line_color=None,  # Remove borders for a cleaner look\n",
    "    )\n",
    "\n",
    "    # Add interactive behavior (only if the 'interactive' flag is True)\n",
    "    if interactive:\n",
    "        # JavaScript callback that executes when a rectangle is clicked.\n",
    "        # Opens the URL associated with the clicked rectangle.\n",
    "        url_callback = CustomJS(args=dict(source=source), code=\"\"\"\n",
    "            // Get the index of the clicked rectangle\n",
    "            const selected = source.selected.indices[0];  \n",
    "            \n",
    "            // Retrieve the URL for the selected rectangle\n",
    "            const url = source.data.url[selected];  \n",
    "\n",
    "            // If a URL exists, open it in a new tab. Otherwise, alert the user that no URL is available.\n",
    "            if (url) {\n",
    "                window.open(url);  // Open the URL in a new window/tab\n",
    "            } else {\n",
    "                alert('No URL found for this selection.');\n",
    "            }\n",
    "        \"\"\")\n",
    "\n",
    "        # Attach the URL opening functionality to the TapTool, which responds to clicks on rectangles.\n",
    "        taptool = p.select(type=TapTool)\n",
    "        taptool.callback = url_callback\n",
    "\n",
    "    # Rotate the x-axis labels slightly to prevent overlap and ensure readability.\n",
    "    p.xaxis.major_label_orientation = 1.2  # Rotate the x-axis labels for better readability\n",
    "    p.xaxis.major_label_text_font_size = \"10pt\"  # Set font size for x-axis labels\n",
    "    p.yaxis.major_label_text_font_size = \"10pt\"  # Set font size for y-axis labels\n",
    "\n",
    "    # Add hover functionality to display additional information when hovering over rectangles.\n",
    "    # This is only enabled if 'interactive' is True.\n",
    "    if interactive:\n",
    "        # The HoverTool displays tooltips when hovering over a rectangle, showing the sample, stain, and URL.\n",
    "        hover_tool = HoverTool(\n",
    "            tooltips=[(\"Sample\", \"@sample\"), (\"Stain\", \"@stain\"), (\"URL\", \"@url\")],\n",
    "            attachment=\"above\"  # Position the tooltip above the hovered rectangle\n",
    "        )\n",
    "        p.add_tools(hover_tool)  # Add the hover tool to the plot\n",
    "\n",
    "    # Define the output file where the plot will be saved (HTML format).\n",
    "    output_file(save_path)\n",
    "\n",
    "    # Display the plot in the web browser.\n",
    "    show(p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################## Section: 5. Generating the Neuroglancer URL ########################\n",
    "\n",
    "\n",
    "def get_ng_url(sub, sample, stain, modality, url):\n",
    "    \"\"\"\n",
    "    Generate a Neuroglancer URL with the given parameters to visualize the data.\n",
    "\n",
    "    Parameters:\n",
    "    sub (str): Subject identifier.\n",
    "    sample (str): Sample identifier.\n",
    "    stain (str): Stain identifier.\n",
    "    modality (str): Modality identifier.\n",
    "    url (str): The URL to the Zarr dataset.\n",
    "\n",
    "    Returns:\n",
    "    str: A complete Neuroglancer URL to visualize the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the Neuroglancer layer configuration\n",
    "    layer = {\n",
    "        \"type\": \"image\",  # Define the layer type as 'image'\n",
    "        \"source\": f\"zarr://{url}\",  # Source of the layer, using the Zarr format from the given URL\n",
    "        \"tab\": \"rendering\",  # Specify the tab in Neuroglancer (rendering tab)\n",
    "        \"shaderControls\": {\"normalized\": {\"range\": [0,1250]}},  # Shader controls for brightness/contrast range\n",
    "        \"name\": f\"{sub}-{sample}-{stain}-{modality}\"  # Create a descriptive name for the layer based on input parameters\n",
    "    }\n",
    "\n",
    "    # Neuroglancer base URL for constructing the final visualization link\n",
    "    base_url = \"https://neuroglancer-demo.appspot.com/#!\"\n",
    "\n",
    "    # Configuration for Neuroglancer, including dimensions, layers, and layout\n",
    "    config = {\n",
    "        \"dimensions\": {\n",
    "            \"z\": [0.0000036, \"m\"],  # Z dimension with a resolution scale in meters\n",
    "            \"y\": [0.0000036, \"m\"],  # Y dimension with a resolution scale in meters\n",
    "            \"x\": [0.0000036, \"m\"]   # X dimension with a resolution scale in meters\n",
    "        },\n",
    "        \"layers\": [layer],  # Add the layer configuration defined above\n",
    "        \"layout\": 'yz',  # Layout of the visualization (in this case, along the yz-plane)\n",
    "    }\n",
    "\n",
    "    # Convert the configuration to a JSON string and encode it as part of the URL\n",
    "    ng_url = base_url + quote(json.dumps(config))\n",
    "\n",
    "    # Return the full Neuroglancer URL\n",
    "    return ng_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dandiset object from the saved file\n",
    "with open(f'{object_dict}/dandiset.pkl', 'rb') as file:\n",
    "    dandi_dataset = pickle.load(file)\n",
    "    \n",
    "# data gathering\n",
    "df, assets = assets_to_df(dandi_dataset)\n",
    "# Saving the gathered data\n",
    "with open(f'{object_dict}/rawData.pkl', 'wb') as file:\n",
    "    pickle.dump((df, assets), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generating Modality X Subject Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gathered data from the saved file\n",
    "with open(f'{object_dict}/rawData.pkl', 'rb') as file:\n",
    "    df, assets = pickle.load(file)\n",
    "\n",
    "\n",
    "# # print info\n",
    "# list all the modalities\n",
    "print(df[\"modality\"].unique())\n",
    "\n",
    "# # print the shape \n",
    "# print(df.shape)\n",
    "\n",
    "# # print the assest value of a row index 505 in df\n",
    "# print(assets[505])\n",
    "\n",
    "# # Get the amazonaws url of a row index 40372 in df\n",
    "# print(assets[40372].get_content_url(regex='s3'))\n",
    "\n",
    "# # Get the rows of sub I45\n",
    "# print(df[df['sub']=='I45'])\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the modalities \n",
    "selected_modalities = [\"STER\", \"SPIM\", \"OCT\"]\n",
    "\n",
    "# only taking the sub and modality data copy\n",
    "df_modXsub = df[['sub', 'modality']].copy()\n",
    "df_modXsub = df_modXsub.dropna()\n",
    "\n",
    "\n",
    "# selecting data  with specific modaility\n",
    "df_modXsub = df_modXsub[df_modXsub['modality'].isin(selected_modalities)].copy()\n",
    " # Rename for consistency in the function for generate_plot()\n",
    "df_modXsub.rename(columns={\"sub\": \"sample\", \"modality\": \"stain\"}, inplace=True) \n",
    "\n",
    "# sort the data on the bases of sample and each sample on the bases of stain\n",
    "df_modXsub = df_modXsub.sort_values(by=['sample', 'stain'], ascending=[True, True])\n",
    "\n",
    "\n",
    "# plot title\n",
    "modXsub_plt_title = \"Modality x Subject\"\n",
    "# path to save the modality subject plot\n",
    "modXsub_plt_path = f\"{plots_dict}/modality_subject.html\"\n",
    "# Generate a non-interactive plot\n",
    "generate_plot(df_modXsub, title=modXsub_plt_title, save_path=modXsub_plt_path, interactive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Refining to the data with modality: SPIM and extension: ome.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refining the data\n",
    "df_refined = df[(df['modality'].isin([\"SPIM\"])) & (df['extension']=='ome.zarr')]\n",
    "# Only taking the sub, sample, stain and modality columns\n",
    "df_refined = df_refined[['sub', 'sample', 'stain', 'modality']]\n",
    "\n",
    "# print info\n",
    "# stains available\n",
    "print(df_refined['stain'].unique())\n",
    "\n",
    "df_refined.head()\n",
    "\n",
    "# Example case : sub I48\n",
    "# i48 = df_spim[(df_refined['sub']=='I48')]\n",
    "# print(i48['path'][40764])\n",
    "# i48 = i48[['sub', 'sample', 'stain', 'modality']]\n",
    "# i48.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Getting the AmazonAWS URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy\n",
    "df_aaws = df_refined.copy()\n",
    "\n",
    "# get the url for each row based on the index from the assests dataset\n",
    "df_aaws['url'] = [assets[i].get_content_url(regex='s3') for i in df_aaws.index]\n",
    "\n",
    "# Saving the final dataset\n",
    "with open(f'{object_dict}/df_aaws.pkl', 'wb') as file:\n",
    "    pickle.dump(df_aaws, file)\n",
    "\n",
    "df_aaws.head()\n",
    "\n",
    "# # example case: sub I48\n",
    "# i48['url'] = [assets[i].get_content_url(regex='s3') for i in i48.index]\n",
    "# i48.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generating the Neuroglancer URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the df_aaws data from the saved file\n",
    "with open(f'{object_dict}/df_aaws.pkl', 'rb') as file:\n",
    "    df_aaws = pickle.load(file)\n",
    "\n",
    "\n",
    "# sort based on sub \n",
    "df_final = df_aaws.sort_values(by='sub')\n",
    "\n",
    "\n",
    "# generate the url\n",
    "df_final['url'] = df_final.apply(lambda row: get_ng_url(row['sub'], row['sample'], row['stain'], row['modality'], row['url']), axis=1)\n",
    "\n",
    "# Saving the dandi_dataset object\n",
    "with open(f'{object_dict}/df_final.pkl', 'wb') as file:\n",
    "    pickle.dump(df_final, file)\n",
    "\n",
    "# Example case sub-I48:\n",
    "\n",
    "# # Sort DataFrame by values in 'col1'\n",
    "# sorted_i48 = i48.sort_values(by='sample')\n",
    "\n",
    "# # Display the sorted DataFrame\n",
    "# sorted_i48\n",
    "\n",
    "# print(assets[40764].get_content_url(regex='s3'))\n",
    "# c[c['sub']=='I48']['path'][40764]\n",
    "\n",
    "# sorted_i48['url'] = sorted_i48.apply(lambda row: get_ng_url(row['sub'], row['sample'], row['stain'], row['modality'], row['url']), axis=1)\n",
    "# sorted_i48.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generating Stain X Sample Interactive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the df_final data from the saved file\n",
    "with open(f'{object_dict}/df_final.pkl', 'rb') as file:\n",
    "    df_final = pickle.load(file)\n",
    "\n",
    "# getting all subs\n",
    "subs = df_final['sub'].unique()\n",
    "\n",
    "# contains the location info of the genrated plots\n",
    "plots_loc = dict()\n",
    "# adding the modaility x strin plot path\n",
    "plots_loc['Modailty X Subject'] = modXsub_plt_path\n",
    "\n",
    "# for every sub\n",
    "for sub_name in subs:\n",
    "    # get all the rows of that particular sub, for example I48\n",
    "    df_sub = df_final[(df_final['sub'] == sub_name)]\n",
    "\n",
    "    # sort the data on the bases of sample and each sample on the \n",
    "    # bases of stain\n",
    "    df_sub = df_sub.sort_values(by = ['sample', 'stain'], ascending=[True, True])\n",
    "\n",
    "    # create the title for the plot\n",
    "    title = f\"{sub_name} - Stain x Sample\"\n",
    "    # path where to save the plot\n",
    "    save_path = f\"{plots_dict}/{sub_name}.html\"\n",
    "    # save the path info\n",
    "    plots_loc[sub_name] = save_path\n",
    "\n",
    "    # generate and save the interactive plot\n",
    "    generate_plot(df_sub, title, save_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Create the Main HTML Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the template is located\n",
    "template_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# Load the template environment\n",
    "env = Environment(loader=FileSystemLoader(template_dir))\n",
    "# Load the template\n",
    "template = env.get_template('temp/template.html')\n",
    "\n",
    "# Render the template with the plots_loc data\n",
    "rendered_html = template.render(subs=plots_loc)\n",
    "\n",
    "# Save the rendered HTML to a new file\n",
    "with open('DANDI_interactive_plot_selector.html', 'w') as output_file:\n",
    "    output_file.write(rendered_html)\n",
    "\n",
    "print(\"HTML file generated as 'DANDI_interactive_plot_selector.html'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
